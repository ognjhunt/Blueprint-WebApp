You are absolutely right to point that out! My apologies. I missed a crucial detail in my previous responses. You're not currently using the Gemini 2.0 Flash model. You're using the gemini-pro-vision model, which is a different model, and not the "flash" variant.

Here's why and how to fix it:

The Issue:

Model Endpoint: You are using the https://generativelanguage.googleapis.com/v1/models/gemini-pro-vision:generateContent endpoint. This endpoint is for the standard gemini-pro-vision model, not the gemini-2.0-flash-exp model.* No Explicit Flash Model: There is no explicit mention of the gemini-2.0-flash-exp model in your code.

How to Use Gemini 2.0 Flash:

Model Name: You need to use the correct model name in your API call. The Gemini 2.0 Flash model is typically referred to as gemini-2.0-flash-exp (or a similar variant, depending on the specific API version).2. Vertex AI Endpoint: When using Vertex AI, you need to use the Vertex AI endpoint for the Gemini model. The endpoint will be different from the one you are currently using.

Corrected Code (API Route app/api/gemini/route.ts):

// app/api/gemini/route.ts
import { NextResponse } from "next/server";
import { VertexAI } from "@google-cloud/aiplatform";

export async function POST(req: Request) {
  try {
    const { imageUrl } = await req.json();

    if (!imageUrl) {
      return NextResponse.json({ error: "Image URL is required" }, { status: 400 });
    }

    const base64Image = await convertImageToBase64(imageUrl);
    if (!base64Image) {
      return NextResponse.json({ error: "Failed to convert image to base64" }, { status: 500 });
    }

    // Initialize Vertex AI
    const vertexAI = new VertexAI({
        project: process.env.GCP_PROJECT_ID || "", // Replace with your project ID
        location: process.env.GCP_LOCATION || "us-central1", // Replace with your location
    });

    // Get the Gemini Pro Vision model
    const model = vertexAI.preview.getGenerativeModel({
        model: "gemini-2.0-flash-exp", // Use the Gemini 2.0 Flash model
    });

    const request = {
        contents: [
            {
                parts: [
                    {
                        text: "Analyze this floor plan and provide insights about the layout, potential hotspots, and suggestions for improvement. Focus on spatial organization, traffic flow, and areas that could benefit from AR enhancements.",
                    },
                    {
                        inlineData: {
                            mimeType: "image/jpeg",
                            data: base64Image,
                        },
                    },
                ],
            },
        ],
    };

    const response = await model.generateContent(request);

    const analysis = response.response.text();

    return NextResponse.json({ analysis });
  } catch (error: any) {
    console.error("Error in API route:", error);
    return NextResponse.json({ error: error.message || "An unexpected error occurred" }, { status: 500 });
  }
}

async function convertImageToBase64(imageUrl: string): Promise<string | null> {
  try {
    const response = await fetch(imageUrl, {
      mode: "no-cors",
      cache: "no-cache",
      headers: {
        "Access-Control-Allow-Origin": "*",
      },
    });

    if (!response.ok) {
      console.error("Response not ok:", response.status, response.statusText);
      throw new Error("Failed to fetch image: " + response.statusText);
    }

    const blob = await response.blob();
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onloadend = () => {
        if (reader.result && typeof reader.result === "string") {
          // Get the base64 data after the "base64," marker
          const base64Data = reader.result.split(",")[1];
          resolve(base64Data);
        } else {
          reject(new Error("Failed to convert to base64"));
        }
      };
      reader.onerror = (error) => {
        console.error("FileReader error:", error);
        reject(new Error("Failed to read image file"));
      };
      reader.readAsDataURL(blob);
    });
  } catch (error) {
    console.error("Error converting image to base64:", error);
    return null;
  }
}
Use code with caution.
TypeScript
Key Changes in the API Route:

Vertex AI Initialization: typescript const vertexAI = new VertexAI({ project: process.env.GCP_PROJECT_ID || "", // Replace with your project ID location: process.env.GCP_LOCATION || "us-central1", // Replace with your location }); This initializes the Vertex AI client with your project ID and location. Make sure to set the GCP_PROJECT_ID and GCP_LOCATION environment variables.* Get Gemini Flash Model: typescript const model = vertexAI.preview.getGenerativeModel({ model: "gemini-2.0-flash-exp", // Use the Gemini 2.0 Flash model }); This retrieves the gemini-2.0-flash-exp model from Vertex AI.* generateContent Method: typescript const response = await model.generateContent(request); This uses the generateContent method of the Vertex AI model to send the request.* Response Extraction: ```typescript
const analysis = response.response.text();

Use code with caution.
This extracts the text from the response.
Important Notes:

Environment Variables: Make sure you have the following environment variables set: * GEMINI_API_KEY: (This is no longer used in this version of the code, but keep it for other parts of your app) * GCP_PROJECT_ID: Your Google Cloud project ID. * GCP_LOCATION: The location of your Vertex AI resources (e.g., us-central1).* Vertex AI SDK: Ensure you have the @google-cloud/aiplatform package installed.* Authentication: Ensure your application has the correct authentication to access Vertex AI. This is typically done using a service account key.

Client-Side Changes (React Component):

You don't need to change much in your React component, as the API call is now handled by the API route. Your analyzeFloorPlanWithGemini function should now look like this:

async function analyzeFloorPlanWithGemini(floorPlanUrl: string) {
    if (!floorPlanUrl) return;
    setIsAnalyzing(true);
    try {
      const response = await fetch("/api/gemini", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({ imageUrl: floorPlanUrl }),
      });

      if (!response.ok) {
        const errorData = await response.json();
        throw new Error(
          `Gemini analysis request failed: ${response.status} - ${errorData.error}`,
        );
      }

      const data = await response.json();
      setGeminiAnalysis(data.analysis);
      toast({
        title: "Analysis Complete",
        description: "Floor plan analysis has been generated successfully.",
      });
    } catch (error) {
      console.error("Error:", error);
      toast({
        title: "Analysis Failed",
        description:
          error instanceof Error
            ? error.message
            : "Failed to analyze floor plan",
        variant: "destructive",
      });
    } finally {
      setIsAnalyzing(false);
    }
  }
Use code with caution.
TypeScript
Key Changes in the Client-Side:

Fetch to API Route: The fetch request is now made to /api/gemini instead of the Gemini API endpoint directly.* Request Body: The request body now includes the imageUrl.* Response Extraction: The analysis text is extracted from the data.analysis property of the response.

Summary of Changes:

API Route: * Uses the Vertex AI SDK to interact with the gemini-2.0-flash-exp model. * Handles the API key securely on the server. * Returns the analysis text as a JSON response.* React Component: * Fetches data from the API route instead of directly calling the Gemini API. * Extracts the analysis text from the response.

By making these changes, you will now be using the Gemini 2.0 Flash model through the Vertex AI API, and your API key will be secure.

Let me know if you have any other questions!